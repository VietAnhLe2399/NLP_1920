{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyphrase Extraction by singleTextRank - pke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year                                              title event_type  \\\n",
       "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
       "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
       "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
       "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
       "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
       "\n",
       "                                            pdf_name          abstract  \\\n",
       "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
       "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
       "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
       "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
       "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
       "\n",
       "                                          paper_text  \n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
       "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
       "4  Neural Network Ensembles, Cross\\nValidation, a...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('papers.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "##Creating a list of custom stopwords\n",
    "new_words = [\"fig\",\"figure\",\"image\",\"sample\",\"using\", \n",
    "             \"show\", \"result\", \"large\", \n",
    "             \"also\", \"one\", \"two\", \"three\", \n",
    "             \"four\", \"five\", \"seven\",\"eight\",\"nine\"]\n",
    "stop_words = list(stop_words.union(new_words))\n",
    "\n",
    "def pre_process(text):\n",
    "    \n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    ##Convert to list from string\n",
    "    text = text.split()\n",
    "    \n",
    "    # remove stopwords\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "\n",
    "    # remove words less than three letters\n",
    "    text = [word for word in text if len(word) >= 3]\n",
    "\n",
    "    # lemmatize\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    text = [lmtzr.lemmatize(word) for word in text]\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pre_process(df['paper_text'][941])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Title=====\n",
      "Algorithms for Non-negative Matrix Factorization\n",
      "\n",
      "=====Abstract=====\n",
      "Non-negative matrix factorization (NMF) has previously been shown to \n",
      "be a useful decomposition for multivariate data. Two different multi- \n",
      "plicative algorithms for NMF are analyzed. They differ only slightly in \n",
      "the multiplicative factor used in the update rules. One algorithm can be \n",
      "shown to minimize the conventional least squares error while the other \n",
      "minimizes the generalized Kullback-Leibler divergence. The monotonic \n",
      "convergence of both algorithms can be proven using an auxiliary func- \n",
      "tion analogous to that used for proving convergence of the Expectation- \n",
      "Maximization algorithm. The algorithms can also be interpreted as diag- \n",
      "onally rescaled gradient descent, where the rescaling factor is optimally \n",
      "chosen to ensure convergence. \n",
      "\n",
      "===Keywords===\n",
      "update rule att att wiavitt itt wia wia hattvitt itt divergence invariant update ifw stationary point divergence proof theorem\n",
      "algorithm non negative matrix factorization daniel lee belj laboratory lucent technology murray hill sebastian seung tdept brain cog sci massachusetts institute technology cambridge\n",
      "result multiplicative factor positive component gradient denominator absolute value negative component numerator factor divergence\n",
      "multiplicative rule positive component gradient denominator negative component numerator multiplicative factor\n",
      "appropriate auxiliary function iiv hii update rule theorem\n",
      "wtwh att wia wia whht euclidean distance invariant update wand stationary point distance theorem\n",
      "non negative matrix factorization course type matrix factorization\n",
      "optimal nonnegative factor data non negative matrix factorization\n",
      "update rule algorithm shown minimize conventional least square error minimizes\n",
      "method disadvantage sensitive choice step size inconvenient application multiplicative update rule\n"
     ]
    }
   ],
   "source": [
    "text = df['paper_text'][941]\n",
    "\n",
    "# define the set of valid Part-of-Speeches\n",
    "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "\n",
    "# 1. create a SingleRank extractor.\n",
    "extractor = pke.unsupervised.SingleRank()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=docs,\n",
    "                        max_length=10000000000,\n",
    "                        language='en',\n",
    "                        normalization=None)\n",
    "\n",
    "# 3. select the longest sequences of nouns and adjectives as candidates.\n",
    "extractor.candidate_selection(pos=pos)\n",
    "\n",
    "# 4. weight the candidates using the sum of their word's scores that are\n",
    "#    computed using random walk. In the graph, nodes are words of\n",
    "#    certain part-of-speech (nouns and adjectives) that are connected if\n",
    "#    they occur in a window of 10 words.\n",
    "extractor.candidate_weighting(window=10,\n",
    "                              pos=pos)\n",
    "\n",
    "# 5. get the 10-highest scored candidates as keyphrases\n",
    "keyphrases = extractor.get_n_best(n=10)\n",
    "\n",
    "idx = 941\n",
    "# now print the results\n",
    "print(\"\\n=====Title=====\")\n",
    "print(df['title'][idx])\n",
    "print(\"\\n=====Abstract=====\")\n",
    "print(df['abstract'][idx])\n",
    "print(\"\\n===Keywords===\")\n",
    "for k in keyphrases:\n",
    "    print(k[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = open(\"C:/Users/vieta/Downloads/dataNLP/keyword-extraction-datasets-master/citeulike180/documents/99.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='C:/Users/vieta/Downloads/dataNLP/keyword-extraction-datasets-master/citeulike180/documents/99.txt' mode='r' encoding='cp1252'>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = da.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"letters to nature\\ntypically slower than 1 km s-1) might differ significantly from what is assumed by current modelling efforts27. The expected equation-of-state differences among small bodies (ice versus rock, for instance) presents another dimension of study; having recently adapted our code for massively parallel architectures (K. M. Olson and E.A, manuscript in preparation), we are now ready to perform a more comprehensive analysis. The exploratory simulations presented here suggest that when a young, non-porous asteroid (if such exist) suffers extensive impact damage, the resulting fracture pattern largely defines the asteroid's response to future impacts. The stochastic nature of collisions implies that small asteroid interiors may be as diverse as their shapes and spin states. Detailed numerical simulations of impacts, using accurate shape models and rheologies, could shed light on how asteroid collisional response depends on internal configuration and shape, and hence on how planetesimals evolve. Detailed simulations are also required before one can predict the quantitative effects of nuclear explosions on Earth-crossing comets and asteroids, either for hazard mitigation28 through disruption and deflection, or for resource exploitation29. Such predictions would require detailed reconnaissance concerning the composition and internal structure of the targeted object.\\nReceived 4 February; accepted 18 March 1998. 1. Asphaug, E. & Melosh, H. J. The Stickney impact of Phobos: A dynamical model. Icarus 101, 144\\xad164 (1993). 2. Asphaug, E. et al. Mechanical and geological effects of impact cratering on Ida. Icarus 120, 158\\xad184 (1996). 3. Nolan, M. C., Asphaug, E., Melosh, H. J. & Greenberg, R. Impact craters on asteroids: Does strength or gravity control their size? Icarus 124, 359\\xad371 (1996). 4. Love, S. J. & Ahrens, T. J. Catastrophic impacts on gravity dominated asteroids. Icarus 124, 141\\xad155 (1996). 5. Melosh, H. J. & Ryan, E. V. Asteroids: Shattered but not dispersed. Icarus 129, 562\\xad564 (1997). 6. Housen, K. R., Schmidt, R. M. & Holsapple, K. A. Crater ejecta scaling laws: Fundamental forms based on dimensional analysis. J. Geophys. Res. 88, 2485\\xad2499 (1983). 7. Holsapple, K. A. & Schmidt, R. M. Point source solutions and coupling parameters in cratering mechanics. J. Geophys. Res. 92, 6350\\xad6376 (1987). 8. Housen, K. R. & Holsapple, K. A. On the fragmentation of asteroids and planetary satellites. Icarus 84, 226\\xad253 (1990). 9. Benz, W. & Asphaug, E. Simulations of brittle solids using smooth particle hydrodynamics. Comput. Phys. Commun. 87, 253\\xad265 (1995). 10. Asphaug, E. et al. Mechanical and geological effects of impact cratering on Ida. Icarus 120, 158\\xad184 (1996). 11. Hudson, R. S. & Ostro, S. J. Shape of asteroid 4769 Castalia (1989 PB) from inversion of radar images. Science 263, 940\\xad943 (1994). 12. Ostro, S. J. et al. Asteroid radar astrometry. Astron. J. 102, 1490\\xad1502 (1991). 13. Ahrens, T. J. & O'Keefe, J. D. in Impact and Explosion Cratering (eds Roddy, D. J., Pepin, R. O. & Merrill, R. B.) 639\\xad656 (Pergamon, New York, 1977). 14. Tillotson, J. H. Metallic equations of state for hypervelocity impact. (General Atomic Report GA-3216, San Diego, 1962). 15. Nakamura, A. & Fujiwara, A. Velocity distribution of fragments formed in a simulated collisional disruption. Icarus 92, 132\\xad146 (1991). 16. Benz, W. & Asphaug, E. Simulations of brittle solids using smooth particle hydrodynamics. Comput. Phys. Commun. 87, 253\\xad265 (1995). 17. Bottke, W. F., Nolan, M. C., Greenberg, R. & Kolvoord, R. A. Velocity distributions among colliding asteroids. Icarus 107, 255\\xad268 (1994). 18. Belton, M. J. S. et al. Galileo encounter with 951 Gaspra--First pictures of an asteroid. Science 257, 1647\\xad1652 (1992). 19. Belton, M. J. S. et al. Galileo's encounter with 243 Ida: An overview of the imaging experiment. Icarus 120, 1\\xad19 (1996). 20. Asphaug, E. & Melosh, H. J. The Stickney impact of Phobos: A dynamical model. Icarus 101, 144\\xad164 (1993). 21. Asphaug, E. et al. Mechanical and geological effects of impact cratering on Ida. Icarus 120, 158\\xad184 (1996). 22. Housen, K. R., Schmidt, R. M. & Holsapple, K. A. Crater ejecta scaling laws: Fundamental forms based on dimensional analysis. J. Geophys. Res. 88, 2485\\xad2499 (1983). 23. Veverka, J. et al. NEAR's flyby of 253 Mathilde: Images of a C asteroid. Science 278, 2109\\xad2112 (1997). 24. Asphaug, E. et al. Impact evolution of icy regoliths. Lunar Planet. Sci. Conf. (Abstr.) XXVIII, 63\\xad64 (1997). ¨ 25. Love, S. G., Horz, F. & Brownlee, D. E. Target porosity effects in impact cratering and collisional disruption. Icarus 105, 216\\xad224 (1993). 26. Fujiwara, A., Cerroni, P., Davis, D. R., Ryan, E. V. & DiMartino, M. in Asteroids II (eds Binzel, R. P., Gehrels, T. & Matthews, A. S.) 240\\xad265 (Univ. Arizona Press, Tucson, 1989). 27. Davis, D. R. & Farinella, P. Collisional evolution of Edgeworth-Kuiper Belt objects. Icarus 125, 50\\xad60 (1997). 28. Ahrens, T. J. & Harris, A. W. Deflection and fragmentation of near-Earth asteroids. Nature 360, 429\\xad 433 (1992). 29. Resources of Near-Earth Space (eds Lewis, J. S., Matthews, M. S. & Guerrieri, M. L.) (Univ. Arizona Press, Tucson, 1993). Acknowledgements. This work was supported by NASA's Planetary Geology and Geophysics Program. Correspondence and requests for materials should be addressed to E.A. (e-mail: asphaug@earthsci.ucsc. edu). * Present address: Paul F. Lazarsfeld Center for the Social Sciences, Columbia University, 812 SIPA Building, 420 W118 St, New York, New York 10027, USA.\\n\\nCollective dynamics of `small-world' networks\\nDuncan J. Watts* & Steven H. Strogatz\\nDepartment of Theoretical and Applied Mechanics, Kimball Hall, Cornell University, Ithaca, New York 14853, USA\\n.........................................................................................................................\\n\\n8\\n\\nNetworks of coupled dynamical systems have been used to model biological oscillators1\\xad4, Josephson junction arrays5,6, excitable media7, neural networks8\\xad10, spatial games11, genetic control networks12 and many other self-organizing systems. Ordinarily, the connection topology is assumed to be either completely regular or completely random. But many biological, technological and social networks lie somewhere between these two extremes. Here we explore simple models of networks that can be tuned through this middle ground: regular networks `rewired' to introduce increasing amounts of disorder. We find that these systems can be highly clustered, like regular lattices, yet have small characteristic path lengths, like random graphs. We call them `small-world' networks, by analogy with the small-world phenomenon13,14 (popularly known as six degrees of separation15). The neural network of the worm Caenorhabditis elegans, the power grid of the western United States, and the collaboration graph of film actors are shown to be small-world networks. Models of dynamical systems with small-world coupling display enhanced signal-propagation speed, computational power, and synchronizability. In particular, infectious diseases spread more easily in small-world networks than in regular lattices. To interpolate between regular and random networks, we consider the following random rewiring procedure (Fig. 1). Starting from a ring lattice with n vertices and k edges per vertex, we rewire each edge at random with probability p. This construction allows us to `tune' the graph between regularity (p ¼ 0) and disorder (p ¼ 1), and thereby to probe the intermediate region 0 p 1, about which little is known. We quantify the structural properties of these graphs by their characteristic path length L(p) and clustering coefficient C(p), as defined in Fig. 2 legend. Here L(p) measures the typical separation between two vertices in the graph (a global property), whereas C(p) measures the cliquishness of a typical neighbourhood (a local property). The networks of interest to us have many vertices with sparse connections, but not so sparse that the graph is in danger of becoming disconnected. Specifically, we require n q k q lnðnÞ q 1, where k q lnðnÞ guarantees that a random graph will be connected16. In this regime, we find that L n=2k q 1 and C 3=4 as p  0, while L Lrandom lnðnÞ=lnðkÞ and C C random k=n p 1 as p  1. Thus the regular lattice at p ¼ 0 is a highly clustered, large world where L grows linearly with n, whereas the random network at p ¼ 1 is a poorly clustered, small world where L grows only logarithmically with n. These limiting cases might lead one to suspect that large C is always associated with large L, and small C with small L. On the contrary, Fig. 2 reveals that there is a broad interval of p over which L(p) is almost as small as Lrandom yet CðpÞ q Crandom . These small-world networks result from the immediate drop in L(p) caused by the introduction of a few long-range edges. Such `short cuts' connect vertices that would otherwise be much farther apart than Lrandom. For small p, each short cut has a highly nonlinear effect on L, contracting the distance not just between the pair of vertices that it connects, but between their immediate neighbourhoods, neighbourhoods of neighbourhoods and so on. By contrast, an edge\\n\\n440\\n\\nNature © Macmillan Publishers Ltd 1998\\n\\nNATURE | VOL 393 | 4 JUNE 1998\\n\\n\\x0cletters to nature\\nremoved from a clustered neighbourhood to make a short cut has, at most, a linear effect on C; hence C(p) remains practically unchanged for small p even though L(p) drops rapidly. The important implication here is that at the local level (as reflected by C(p)), the transition to a small world is almost undetectable. To check the robustness of these results, we have tested many different types of initial regular graphs, as well as different algorithms for random rewiring, and all give qualitatively similar results. The only requirement is that the rewired edges must typically connect vertices that would otherwise be much farther apart than Lrandom. The idealized construction above reveals the key role of short cuts. It suggests that the small-world phenomenon might be common in sparse networks with many vertices, as even a tiny fraction of short cuts would suffice. To test this idea, we have computed L and C for the collaboration graph of actors in feature films (generated from data available at http://us.imdb.com), the electrical power grid of the western United States, and the neural network of the nematode worm C. elegans17. All three graphs are of scientific interest. The graph of film actors is a surrogate for a social network18, with the advantage of being much more easily specified. It is also akin to the graph of mathematical collaborations centred, ¨ traditionally, on P. Erdos (partial data available at http:// www.acs.oakland.edu/ grossman/erdoshp.html). The graph of the power grid is relevant to the efficiency and robustness of power networks19. And C. elegans is the sole example of a completely mapped neural network. Table 1 shows that all three graphs are small-world networks. These examples were not hand-picked; they were chosen because of their inherent interest and because complete wiring diagrams were available. Thus the small-world phenomenon is not merely a curiosity of social networks13,14 nor an artefact of an idealized model--it is probably generic for many large, sparse networks found in nature. We now investigate the functional significance of small-world connectivity for dynamical systems. Our test case is a deliberately simplified model for the spread of an infectious disease. The population structure is modelled by the family of graphs described in Fig. 1. At time t ¼ 0, a single infective individual is introduced into an otherwise healthy population. Infective individuals are removed permanently (by immunity or death) after a period of sickness that lasts one unit of dimensionless time. During this time, each infective individual can infect each of its healthy neighbours with probability r. On subsequent time steps, the disease spreads along the edges of the graph until it either infects the entire population, or it dies out, having infected some fraction of the population in the process.\\n\\n8\\n\\nTable 1 Empirical examples of small-world networks\\n.............................................................................................................................................................................\\n\\nLactual\\n\\nLrandom 2.99 12.4 2.25\\n\\nCactual\\n\\nCrandom\\n\\n............................................................................................................................................................................. Characteristic path length L and clustering coefficient C for three real networks, compared to random graphs with the same number of vertices (n) and average number of edges per vertex (k). (Actors: n ¼ 225;226, k ¼ 61. Power grid: n ¼ 4;941, k ¼ 2:67. C. elegans: n ¼ 282, k ¼ 14.) The graphs are defined as follows. Two actors are joined by an edge if they have acted in a film together. We restrict attention to the giant connected component16 of this graph, which includes 90% of all actors listed in the Internet Movie Database (available at http://us.imdb.com), as of April 1997. For the power grid, vertices represent generators, transformers and substations, and edges represent high-voltage transmission lines between them. For C. elegans, an edge joins two neurons if they are connected by either a synapse or a gap junction. We treat all edges as undirected and unweighted, and all vertices as identical, recognizing that these are crude approximations. All three networks show the small-world phenomenon: L Lrandom but C q Crandom .\\n\\nFilm actors Power grid C. elegans\\n\\n3.65 18.7 2.65\\n\\n0.79 0.080 0.28\\n\\n0.00027 0.005 0.05\\n\\n1\\n\\nRegular\\n\\nSmall-world\\n\\nRandom\\n\\n0.8\\n\\nC(p) / C(0)\\n\\n0.6\\n\\n0.4\\n\\n0.2\\n\\nL(p) / L(0)\\n\\np=0\\nIncreasing randomness\\n\\np=1\\n\\n0 0.0001\\n\\n0.001\\n\\n0.01\\n\\n0.1\\n\\n1\\n\\np\\nFigure 2 Characteristic path length L(p) and clustering coefficient C(p) for the family of randomly rewired graphs described in Fig. 1. Here L is defined as the number of edges in the shortest path between two vertices, averaged over all pairs of vertices. The clustering coefficient C(p) is defined as follows. Suppose that a vertex v has kv neighbours; then at most kv ðkv 1Þ=2 edges can exist between them (this occurs when every neighbour of v is connected to every other neighbour of v). Let Cv denote the fraction of these allowable edges that actually exist. Define C as the average of Cv over all v. For friendship networks, these statistics have intuitive meanings: L is the average number of friendships in the shortest chain connecting two people; Cv reflects the extent to which friends of v are also friends of each other; and thus C measures the cliquishness of a typical friendship circle. The data shown in the figure are averages over 20 random realizations of the rewiring process described in Fig.1, and have been normalized by the values L(0), C(0) for a regular lattice. All the graphs have n ¼ 1;000 vertices and an average degree of k ¼ 10 edges per vertex. We note that a logarithmic horizontal scale has been used to resolve the rapid drop in L(p), corresponding to the onset of the small-world phenomenon. During this drop, C(p) remains almost constant at its value for the regular lattice, indicating that the transition to a small world is almost undetectable at the local level.\\n\\nFigure 1 Random rewiring procedure for interpolating between a regular ring lattice and a random network, without altering the number of vertices or edges in the graph. We start with a ring of n vertices, each connected to its k nearest neighbours by undirected edges. (For clarity, n ¼ 20 and k ¼ 4 in the schematic examples shown here, but much larger n and k are used in the rest of this Letter.) We choose a vertex and the edge that connects it to its nearest neighbour in a clockwise sense. With probability p, we reconnect this edge to a vertex chosen uniformly at random over the entire ring, with duplicate edges forbidden; otherwise we leave the edge in place. We repeat this process by moving clockwise around the ring, considering each vertex in turn until one lap is completed. Next, we consider the edges that connect vertices to their second-nearest neighbours clockwise. As before, we randomly rewire each of these edges with probability p, and continue this process, circulating around the ring and proceeding outward to more distant neighbours after each lap, until each edge in the original lattice has been considered once. (As there are nk/2 edges in the entire graph, the rewiring process stops after k/2 laps.) Three realizations of this process are shown, for different values of p. For p ¼ 0, the original ring is unchanged; as p increases, the graph becomes increasingly disordered until for p ¼ 1, all edges are rewired randomly. One of our main results is that for intermediate values of p, the graph is a small-world network: highly clustered like a regular graph, yet with small characteristic path length, like a random graph. (See Fig. 2.) NATURE | VOL 393 | 4 JUNE 1998\\n\\nNature © Macmillan Publishers Ltd 1998\\n\\n441\\n\\n\\x0cletters to nature\\nTwo results emerge. First, the critical infectiousness rhalf, at which the disease infects half the population, decreases rapidly for small p (Fig. 3a). Second, for a disease that is sufficiently infectious to infect the entire population regardless of its structure, the time T(p) required for global infection resembles the L(p) curve (Fig. 3b). Thus, infectious diseases are predicted to spread much more easily and quickly in a small world; the alarming and less obvious point is how few short cuts are needed to make the world small. Our model differs in some significant ways from other network models of disease spreading20\\xad24. All the models indicate that network structure influences the speed and extent of disease transmission, but our model illuminates the dynamics as an explicit function of structure (Fig. 3), rather than for a few particular topologies, such as random graphs, stars and chains20\\xad23. In the work closest to ours, Kretschmar and Morris24 have shown that increases in the number of concurrent partnerships can significantly accelerate the propagation of a sexually-transmitted disease that spreads along the edges of a graph. All their graphs are disconnected because they fix the average number of partners per person at k ¼ 1. An increase in the number of concurrent partnerships causes faster spreading by increasing the number of vertices in the graph's largest connected component. In contrast, all our graphs are connected; hence the predicted changes in the spreading dynamics are due to more subtle structural features than changes in connectedness. Moreover, changes in the number of concurrent partners are obvious to an individual, whereas transitions leading to a smaller world are not. We have also examined the effect of small-world connectivity on three other dynamical systems. In each case, the elements were coupled according to the family of graphs described in Fig. 1. (1) For cellular automata charged with the computational task of density classification25, we find that a simple `majority-rule' running on a small-world graph can outperform all known human and genetic algorithm-generated rules running on a ring lattice. (2) For the iterated, multi-player `Prisoner's dilemma'11 played on a graph, we find that as the fraction of short cuts increases, cooperation is less likely to emerge in a population of players using a generalized `titfor-tat'26 strategy. The likelihood of cooperative strategies evolving out of an initial cooperative/non-cooperative mix also decreases with increasing p. (3) Small-world networks of coupled phase oscillators synchronize almost as readily as in the mean-field model2, despite having orders of magnitude fewer edges. This result may be relevant to the observed synchronization of widely separated neurons in the visual cortex27 if, as seems plausible, the brain has a small-world architecture. We hope that our work will stimulate further studies of smallworld networks. Their distinctive combination of high clustering with short characteristic path length cannot be captured by traditional approximations such as those based on regular lattices or random graphs. Although small-world architecture has not received much attention, we suggest that it will probably turn out to be widespread in biological, social and man-made systems, often with important dynamical consequences.\\nReceived 27 November 1997; accepted 6 April 1998. 1. Winfree, A. T. The Geometry of Biological Time (Springer, New York, 1980). 2. Kuramoto, Y. Chemical Oscillations, Waves, and Turbulence (Springer, Berlin, 1984). 3. Strogatz, S. H. & Stewart, I. Coupled oscillators and biological synchronization. Sci. Am. 269(6), 102\\xad 109 (1993). 4. Bressloff, P. C., Coombes, S. & De Souza, B. Dynamics of a ring of pulse-coupled oscillators: a group theoretic approach. Phys. Rev. Lett. 79, 2791\\xad2794 (1997). 5. Braiman, Y., Lindner, J. F. & Ditto, W. L. Taming spatiotemporal chaos with disorder. Nature 378, 465\\xad467 (1995). 6. Wiesenfeld, K. New results on frequency-locking dynamics of disordered Josephson arrays. Physica B 222, 315\\xad319 (1996). 7. Gerhardt, M., Schuster, H. & Tyson, J. J. A cellular automaton model of excitable media including curvature and dispersion. Science 247, 1563\\xad1566 (1990). 8. Collins, J. J., Chow, C. C. & Imhoff, T. T. Stochastic resonance without tuning. Nature 376, 236\\xad238 (1995). 9. Hopfield, J. J. & Herz, A. V. M. Rapid local synchronization of action potentials: Toward computation with coupled integrate-and-fire neurons. Proc. Natl Acad. Sci. USA 92, 6655\\xad6662 (1995). 10. Abbott, L. F. & van Vreeswijk, C. Asynchronous states in neural networks of pulse-coupled oscillators. Phys. Rev. E 48(2), 1483\\xad1490 (1993). 11. Nowak, M. A. & May, R. M. Evolutionary games and spatial chaos. Nature 359, 826\\xad829 (1992). 12. Kauffman, S. A. Metabolic stability and epigenesis in randomly constructed genetic nets. J. Theor. Biol. 22, 437\\xad467 (1969). 13. Milgram, S. The small world problem. Psychol. Today 2, 60\\xad67 (1967). 14. Kochen, M. (ed.) The Small World (Ablex, Norwood, NJ, 1989). 15. Guare, J. Six Degrees of Separation: A Play (Vintage Books, New York, 1990). ´ 16. Bollabas, B. Random Graphs (Academic, London, 1985). 17. Achacoso, T. B. & Yamamoto, W. S. AY's Neuroanatomy of C. elegans for Computation (CRC Press, Boca Raton, FL, 1992). 18. Wasserman, S. & Faust, K. Social Network Analysis: Methods and Applications (Cambridge Univ. Press, 1994). 19. Phadke, A. G. & Thorp, J. S. Computer Relaying for Power Systems (Wiley, New York, 1988). 20. Sattenspiel, L. & Simon, C. P. The spread and persistence of infectious diseases in structured populations. Math. Biosci. 90, 341\\xad366 (1988). 21. Longini, I. M. Jr A mathematical model for predicting the geographic spread of new infectious agents. Math. Biosci. 90, 367\\xad383 (1988). 22. Hess, G. Disease in metapopulation models: implications for conservation. Ecology 77, 1617\\xad1632 (1996). 23. Blythe, S. P., Castillo-Chavez, C. & Palmer, J. S. Toward a unified theory of sexual mixing and pair formation. Math. Biosci. 107, 379\\xad405 (1991). 24. Kretschmar, M. & Morris, M. Measures of concurrency in networks and the spread of infectious disease. Math. Biosci. 133, 165\\xad195 (1996). 25. Das, R., Mitchell, M. & Crutchfield, J. P. in Parallel Problem Solving from Nature (eds Davido, Y., ¨ Schwefel, H.-P. & Manner, R.) 344\\xad353 (Lecture Notes in Computer Science 866, Springer, Berlin, 1994). 26. Axelrod, R. The Evolution of Cooperation (Basic Books, New York, 1984). ¨ 27. Gray, C. M., Konig, P., Engel, A. K. & Singer, W. Oscillatory responses in cat visual cortex exhibit intercolumnar synchronization which reflects global stimulus properties. Nature 338, 334\\xad337 (1989). Acknowledgements. We thank B. Tjaden for providing the film actor data, and J. Thorp and K. Bae for the Western States Power Grid data. This work was supported by the US National Science Foundation (Division of Mathematical Sciences). Correspondence and requests for materials should be addressed to D.J.W. (e-mail: djw24@columbia.edu).\\n\\n8\\n\\na\\n0.35\\n\\n0.3\\n\\nr\\n\\nhalf\\n\\n0.25\\n\\n0.2\\n\\n0.15 0.0001\\n\\n0.001\\n\\n0.01\\n\\n0.1\\n\\n1\\n\\np\\nb\\n1\\n\\n0.8\\n\\nT(p) /T(0) L(p) /L(0)\\n\\n0.6\\n\\n0.4\\n\\n0.2\\n\\n0 0.0001\\n\\n0.001\\n\\n0.01\\n\\n0.1\\n\\n1\\n\\np\\nFigure 3 Simulation results for a simple model of disease spreading. The community structure is given by one realization of the family of randomly rewired graphs used in Fig. 1. a, Critical infectiousness rhalf, at which the disease infects half the population, decreases with p. b, The time T(p) required for a maximally infectious disease (r ¼ 1) to spread throughout the entire population has essentially the same functional form as the characteristic path length L(p). Even if only a few per cent of the edges in the original lattice are randomly rewired, the time to global infection is nearly as short as for a random graph.\\n\\n442\\n\\nNature © Macmillan Publishers Ltd 1998\\n\\nNATURE | VOL 393 | 4 JUNE 1998\\n\\n\\x0c\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Algorithms for Non-negative Matrix\\nFactorization\\n\\nDaniel D. Lee*\\n*BelJ Laboratories\\nLucent Technologies\\nMurray Hill, NJ 07974\\n\\nH. Sebastian Seung*t\\ntDept. of Brain and Cog. Sci.\\nMassachusetts Institute of Technology\\nCambridge, MA 02138\\n\\nAbstract\\nNon-negative matrix factorization (NMF) has previously been shown to\\nbe a useful decomposition for multivariate data. Two different multiplicative algorithms for NMF are analyzed. They differ only slightly in\\nthe multiplicative factor used in the update rules. One algorithm can be\\nshown to minimize the conventional least squares error while the other\\nminimizes the generalized Kullback-Leibler divergence. The monotonic\\nconvergence of both algorithms can be proven using an auxiliary function analogous to that used for proving convergence of the ExpectationMaximization algorithm. The algorithms can also be interpreted as diagonally rescaled gradient descent, where the rescaling factor is optimally\\nchosen to ensure convergence.\\n\\n1 Introduction\\nUnsupervised learning algorithms such as principal components analysis and vector quantization can be understood as factorizing a data matrix subject to different constraints. Depending upon the constraints utilized, the resulting factors can be shown to have very different representational properties. Principal components analysis enforces only a weak orthogonality constraint, resulting in a very distributed representation that uses cancellations\\nto generate variability [1, 2]. On the other hand, vector quantization uses a hard winnertake-all constraint that results in clustering the data into mutually exclusive prototypes [3].\\nWe have previously shown that nonnegativity is a useful constraint for matrix factorization\\nthat can learn a parts representation of the data [4, 5]. The nonnegative basis vectors that are\\nlearned are used in distributed, yet still sparse combinations to generate expressiveness in\\nthe reconstructions [6, 7]. In this submission, we analyze in detail two numerical algorithms\\nfor learning the optimal nonnegative factors from data.\\n\\n2 Non-negative matrix factorization\\nWe formally consider algorithms for solving the following problem:\\nNon-negative matrix factorization (NMF) Given a non-negative matrix\\nV, find non-negative matrix factors Wand H such that:\\nV~WH\\n\\n(1)\\n\\n\\x0cNMF can be applied to the statistical analysis of multivariate data in the following manner.\\nGiven a set of of multivariate n-dimensional data vectors, the vectors are placed in the\\ncolumns of an n x m matrix V where m is the number of examples in the data set. This\\nmatrix is then approximately factorized into an n x r matrix Wand an r x m matrix H.\\nUsually r is chosen to be smaller than nor m , so that Wand H are smaller than the original\\nmatrix V. This results in a compressed version of the original data matrix.\\nWhat is the significance of the approximation in Eq. (1)? It can be rewritten column by\\ncolumn as v ~ Wh, where v and h are the corresponding columns of V and H. In other\\nwords, each data vector v is approximated by a linear combination of the columns of W,\\nweighted by the components of h. Therefore W can be regarded as containing a basis\\nthat is optimized for the linear approximation of the data in V. Since relatively few basis\\nvectors are used to represent many data vectors, good approximation can only be achieved\\nif the basis vectors discover structure that is latent in the data.\\nThe present submission is not about applications of NMF, but focuses instead on the technical aspects of finding non-negative matrix factorizations. Of course, other types of matrix factorizations have been extensively studied in numerical linear algebra, but the nonnegativity constraint makes much of this previous work inapplicable to the present case\\n[8].\\n\\nHere we discuss two algorithms for NMF based on iterative updates of Wand H. Because\\nthese algorithms are easy to implement and their convergence properties are guaranteed,\\nwe have found them very useful in practical applications. Other algorithms may possibly\\nbe more efficient in overall computation time, but are more difficult to implement and may\\nnot generalize to different cost functions. Algorithms similar to ours where only one of the\\nfactors is adapted have previously been used for the deconvolution of emission tomography\\nand astronomical images [9, 10, 11, 12].\\nAt each iteration of our algorithms, the new value of W or H is found by multiplying the\\ncurrent value by some factor that depends on the quality ofthe approximation in Eq. (1). We\\nprove that the quality of the approximation improves monotonically with the application\\nof these multiplicative update rules. In practice, this means that repeated iteration of the\\nupdate rules is guaranteed to converge to a locally optimal matrix factorization.\\n\\n3 Cost functions\\nTo find an approximate factorization V ~ W H, we first need to define cost functions\\nthat quantify the quality of the approximation. Such a cost function can be constructed\\nusing some measure of distance between two non-negative matrices A and B . One useful\\nmeasure is simply the square of the Euclidean distance between A and B [13],\\n\\nIIA - BI12 = L(Aij -\\n\\nBij)2\\n\\n(2)\\n\\nij\\n\\nThis is lower bounded by zero, and clearly vanishes if and only if A = B .\\nAnother useful measure is\\nD(AIIB)\\n\\n=\\n\\n2:\\n\\nk?\\n( Aij log B:~\\n- Aij\\n\\n+ Bij )\\n\\n(3)\\n\\n\"J\\n\\nLike the Euclidean distance this is also lower bounded by zero, and vanishes if and only\\nif A = B . But it cannot be called a \"distance\", because it is not symmetric in A and B,\\nso we will refer to it as the \"divergence\" of A from B. It reduces to the Kullback-Leibler\\ndivergence, or relative entropy, when 2:ij Aij = 2:ij Bij = 1, so that A and B can be\\nregarded as normalized probability distributions.\\n\\n\\x0cWe now consider two alternative formulations of NMF as optimization problems:\\nProblem 1 Minimize\\n\\nIIV -\\n\\nW\\n\\nHI12 with\\n\\nrespect to Wand H, subject to the constraints\\n\\nW,H~O.\\n\\nProblem 2 Minimize D(VIIW H) with re.lpect to Wand H, subject to the constraints\\nW,H~O.\\n\\nAlthough the functions IIV - W HI12 and D(VIIW H) are convex in W only or H only, they\\nare not convex in both variables together. Therefore it is unrealistic to expect an algorithm\\nto solve Problems 1 and 2 in the sense of finding global minima. However, there are many\\ntechniques from numerical optimization that can be applied to find local minima.\\nGradient descent is perhaps the simplest technique to implement, but convergence can be\\nslow. Other methods such as conjugate gradient have faster convergence, at least in the\\nvicinity of local minima, but are more complicated to implement than gradient descent\\n[8] . The convergence of gradient based methods also have the disadvantage of being very\\nsensitive to the choice of step size, which can be very inconvenient for large applications.\\n\\n4 Multiplicative update rules\\nWe have found that the following \"multiplicative update rules\" are a good compromise\\nbetween speed and ease of implementation for solving Problems 1 and 2.\\nTheorem 1 The Euclidean distance II V - W H II is non increasing under the update rules\\n(WTV)att\\nHal\\' +- Hal\\' (WTWH)att\\n\\n(V HT)ia\\nWia +- Wia(WHHT)ia\\n\\n(4)\\n\\nThe Euclidean distance is invariant under these updates if and only if Wand H are at a\\nstationary point of the distance.\\n\\nTheorem 2 The divergence D(VIIW H) is nonincreasing under the update rules\\nH\\n\\natt +-\\n\\nH\\natt\\n\\n2:i WiaVitt/(WH)itt\\n\" W\\nL..Jk\\n\\nka\\n\\nWia +- Wia\\n\\n2:1\\' HattVitt/(WH)itt\\n\" H\\nL..Jv\\n\\nav\\n\\n(5)\\n\\nThe divergence is invariant under these updates if and only ifW and H are at a stationary\\npoint of the divergence.\\n\\nProofs of these theorems are given in a later section. For now, we note that each update\\nconsists of multiplication by a factor. In particular, it is straightforward to see that this\\nmultiplicative factor is unity when V = W H, so that perfect reconstruction is necessarily\\na fixed point of the update rules.\\n\\n5 Multiplicative versus additive update rules\\nIt is useful to contrast these multiplicative updates with those arising from gradient descent\\n[14]. In particular, a simple additive update for H that reduces the squared distance can be\\nwritten as\\n\\n(6)\\nIf \\'flatt are all set equal to some small positive number, this is equivalent to conventional\\ngradient descent. As long as this number is sufficiently small, the update should reduce\\nIIV - WHII?\\n\\n\\x0cNow if we diagonally rescale the variables and set\\n\\nHalt\\n\\n\"Ialt\\n\\n(7)\\n\\n= (WTW H)alt \\'\\n\\nthen we obtain the update rule for H that is given in Theorem 1. Note that this rescaling\\nresults in a multiplicative factor with the positive component of the gradient in the denominator and the absolute value of the negative component in the numerator of the factor.\\nFor the divergence, diagonally rescaled gradient descent takes the form\\n\\nHalt\\n\\nf-\\n\\nHalt\\n\\n+ \"Ialt\\n\\n[~Wia (:;;)ilt - ~ Wia].\\n\\n(8)\\n\\nAgain, if the \"Ialt are small and positive, this update should reduce D (V II W H). If we now\\nset\\n\\nHalt\\n\"Ialt= ui\\n~ W. \\'\\nza\\n\\n(9)\\n\\nthen we obtain the update rule for H that is given in Theorem 2. This rescaling can also\\nbe interpretated as a multiplicative rule with the positive component of the gradient in the\\ndenominator and negative component as the numerator of the multiplicative factor.\\nSince our choices for \"Ialt are not small, it may seem that there is no guarantee that such a\\nrescaled gradient descent should cause the cost function to decrease. Surprisingly, this is\\nindeed the case as shown in the next section.\\n\\n6 Proofs of convergence\\nTo prove Theorems 1 and 2, we will make use of an auxiliary function similar to that used\\nin the Expectation-Maximization algorithm [15, 16].\\nDefinition 1 G(h, h\\') is an auxiliary functionfor F(h)\\nG(h, h\\') ~ F(h),\\n\\nG(h, h)\\n\\nif the conditions\\n\\n= F(h)\\n\\n(10)\\n\\nare satisfied.\\n\\nThe auxiliary function is a useful concept because of the following lemma, which is also\\ngraphically illustrated in Fig. 1.\\nLemma 1 IfG is an auxiliary junction, then F is nonincreasing under the update\\nht+1 = argmlnG (h,ht )\\nProof: F(ht+1) ~ G(ht+1, ht) ~ G(ht, ht)\\n\\n(11)\\n\\n= F(ht) ?\\n\\nNote that F(ht+1) = F(ht) only if ht is a local minimum of G(h, ht). If the derivatives\\nof F exist and are continuous in a small neighborhood of ht , this also implies that the\\nderivatives \\'V F(ht) = O. Thus, by iterating the update in Eq. (11) we obtain a sequence\\nof estimates that converge to a local minimum h min = argminh F(h) of the objective\\nfunction:\\n\\nWe will show that by defining the appropriate auxiliary functions G(h, ht) for both IIV W HII and D(V, W H), the update rules in Theorems 1 and 2 easily follow from Eq. (11).\\n\\n\\x0cFigure 1: Minimizing the auxiliary function G(h, ht)\\nF(ht) for h n+1 = argminh G(h, ht).\\n\\n2:: F(h) guarantees that F(ht+1) :::;\\n\\nLemma 2 If K(ht) is the diagonal matrix\\nKab(ht) = <5ab(WTwht)a/h~\\nthen\\nG(h, ht)\\n\\n= F(ht) + (h -\\n\\n+ ~(h -\\n\\nht)T\\\\l F(ht)\\n\\n(13)\\n\\nht)T K(ht)(h - ht)\\n\\n(14)\\n\\nis an auxiliary function for\\nF(h) =\\n\\n~ ~)Vi -\\n\\na\\n\\nProof: Since G(h, h) = F(h) is obvious, we need only show that G(h, ht)\\ndo this, we compare\\nF(h) = F(ht)\\n\\n+ (h -\\n\\nhtf\\\\l F(ht)\\n\\n+ ~(h -\\n\\n2:: F(h). To\\n\\nht)T(WTW)(h - ht)\\n\\n2\\nwith Eq. (14) to find that G(h, ht) 2:: F(h) is equivalent to\\n\\n0:::;\\n\\n(15)\\n\\nW ia h a )2\\n\\nL\\n\\ni\\n\\n(h - htf[K(ht) - WTW](h - ht)\\n\\n(16)\\n\\n(17)\\n\\nTo prove positive semidefiniteness, consider the matrix 1:\\n(18)\\n\\nwhich is just a rescaling of the components of K - WTW. Then K - WTW is positive\\nsemidefinite if and only if M is, and\\nVT M v\\n\\n=\\n\\nL VaMabVb\\nab\\n\\n(19)\\n\\nL h~(WTW)abh~v~ - vah~(WTW)abh~Vb\\nab\\n\\n(20)\\n\\n\"\\nt t\\nL...J(W T W ) abhahb\\n\\n[1 + 1\\n2\" v a2\\n\\n2\" Vb2 - VaVb ]\\n\\n(21)\\n\\nab\\n\\n= ~ L(WTW)abh~h~(va -\\n\\nVb)2\\n\\n(22)\\n\\nab\\n\\n> 0\\n\\n(23)\\n\\n\\'One can also show that K - WTW is positive semidefinite by considering the matrix K (I1\\n2.\\nThen v. /M(WT W ht ) a is a positive eigenvector of K- 21 W T W K- with\\nunity eigenvalue, and application of the Frobenius-Perron theorem shows that Eq. 17 holds.\\n\\nK- 21 W TW K- 21) K\\n\\n\\x0c?\\n\\nWe can now demonstrate the convergence of Theorem 1:\\nProof of Theorem 1 Replacing G(h, ht) in Eq. (11) by Eq. (14) results in the update rule:\\nht+1 = ht - K(ht)-l\\\\1F(ht)\\n(24)\\nSince Eq. (14) is an auxiliary function, F is nonincreasing under this update rule, according\\nto Lemma 1. Writing the components of this equation explicitly, we obtain\\nht +1 = ht (WT V )a\\na\\na (WTWht)a .\\n\\n(25)\\n\\nBy reversing the roles of Wand H in Lemma 1 and 2, F can similarly be shown to be\\nnonincreasing under the update rules for W .?\\nWe now consider the following auxiliary function for the divergence cost function:\\nLemma 3 Define\\nG(h,ht)\\n\\n(26)\\n\\nia\\n\"\\nWiah~ (\\nWiah~ )\\n- ~ Vi,\"\", W - ht logWiaha -log,\"\", W - ht\\nia\\nub ,b b\\nub ,b b\\n\\n(27)\\n\\nThis is an auxiliary function for\\n\\nF(h)\\n\\n=L\\n\\nVi log\\ni\\n\\n(~ ~_\\na\\n\\n\\'l,a\\n\\nh ) - Vi\\n\\n+ LWiaha\\n\\na\\n\\nProof: It is straightforward to verify that G(h, h) = F(h) . To show that G(h, ht)\\nwe use convexity of the log function to derive the inequality\\nW\\niaha\\n-log \"~ Wiaha ::; - \"\\n~\\nQ a log - a\\n\\n(28)\\n\\na\\n\\n2: F(h),\\n(29)\\n\\nQa\\n\\na\\n\\nwhich holds for all nonnegative Q a that sum to unity. Setting\\nWiah~\\n\\nQ\\n\\na\\n\\n(30)\\n\\n= \\'ub\\n\"\\'\" Wibhbt\\n\\nwe obtain\\n-log \"~ Wiaha ::; - \"~ \\'\"\\'\"Wiah~\\nW- ht ( log Wiaha - log,\"\",Wiah~\\nW- ht )\\na\\na ub ,b b\\nub ,b b\\n\\n(31)\\n\\nFrom this inequality it follows that F(h) ::; G(h, ht) . ?\\nTheorem 2 then follows from the application of Lemma 1:\\nProof of Theorem 2: The minimum of G(h, ht) with respect to h is determined by setting\\nthe gradient to zero:\\n_dG---,(,---,h,_h--,-t) __ \"\\n_ Wiah~ 1\\n~v,\\nt\\ndha\\n_\\n, ~b Wibhb ha\\n\\n\"W- - 0\\n\\n+~\\n\\n,-\\n\\nza-\\n\\n(32)\\n\\nThus, the update rule of Eq. (11) takes the form\\nt+1\\nha\\n\\nh~\"\\n\\nVi\\n\\n= ub\\n\\'\"\\'\" wkb ~\\n\\'\"\\'\" W-,b htbW ia ?\\ni\\nub\\n\\n(33)\\n\\nSince G is an auxiliary function, F in Eq. (28) is nonincreasing under this update. Rewritten in matrix form, this is equivalent to the update rule in Eq. (5). By reversing the roles of\\nHand W, the update rule for W can similarly be shown to be nonincreasing .?\\n\\n\\x0c7 Discussion\\nWe have shown that application of the update rules in Eqs. (4) and (5) are guaranteed to\\nfind at least locally optimal solutions of Problems 1 and 2, respectively. The convergence\\nproofs rely upon defining an appropriate auxiliary function . We are currently working to\\ngeneralize these theorems to more complex constraints. The update rules themselves are\\nextremely easy to implement computationally, and will hopefully be utilized by others for\\na wide variety of applications.\\nWe acknowledge the support of Bell Laboratories. We would also like to thank Carlos\\nBrody, Ken Clarkson, Corinna Cortes, Roland Freund, Linda Kaufman, Yann Le Cun, Sam\\nRowei s, Larry Saul, and Margaret Wright for helpful discussions.\\n\\nReferences\\n[1] Jolliffe, IT (1986). Principal Component Analysis. New York: Springer-Verlag.\\n[2] Turk, M & Pentland, A (1991). Eigenfaces for recognition. J. Cogn. Neurosci. 3, 71- 86.\\n[3] Gersho, A & Gray, RM (1992). Vector Quantization and Signal Compression. Kluwer Acad.\\nPress.\\n[4] Lee, DD & Seung, HS . Unsupervised learning by convex and conic coding (1997). Proceedings\\nof the Conference on Neural Information Processing Systems 9, 515- 521.\\n[5] Lee, DD & Seung, HS (1999). Learning the parts of objects by non-negative matrix factorization. Nature 401, 788- 791.\\n[6] Field, DJ (1994). What is the goal of sensory coding? Neural Comput. 6, 559-601.\\n[7] Foldiak, P & Young, M (1995). Sparse coding in the primate cortex. The Handbook of Brain\\nTheory and Neural Networks, 895- 898. (MIT Press, Cambridge, MA).\\n[8] Press, WH, Teukolsky, SA, Vetterling, WT & Flannery, BP (1993). Numerical recipes: the art\\nof scientific computing. (Cambridge University Press, Cambridge, England).\\n[9] Shepp, LA & Vardi, Y (1982) . Maximum likelihood reconstruction for emission tomography.\\nIEEE Trans . MI-2, 113- 122.\\n[10] Richardson, WH (1972) . Bayesian-based iterative method of image restoration. 1. Opt. Soc.\\nAm. 62, 55- 59.\\n\\n[11] Lucy, LB (1974). An iterative technique for the rectification of observed distributions. Astron.\\nJ. 74, 745- 754.\\n[12] Bouman, CA & Sauer, K (1996). A unified approach to statistical tomography using coordinate\\ndescent optimization. IEEE Trans. Image Proc. 5, 480--492.\\n[13] Paatero, P & Tapper, U (1997). Least squares formulation of robust non-negative factor analysis. Chemometr. Intell. Lab. 37, 23- 35.\\n[14] Kivinen, J & Warmuth, M (1997). Additive versus exponentiated gradient updates for linear\\nprediction. Journal of Tnformation and Computation 132, 1-64.\\n[15] Dempster, AP, Laird, NM & Rubin, DB (1977). Maximum likelihood from incomplete data via\\nthe EM algorithm. J. Royal Stat. Soc. 39, 1-38.\\n[16] Saul, L & Pereira, F (1997). Aggregate and mixed-order Markov models for statistical language\\nprocessing. In C. Cardie and R. Weischedel (eds). Proceedings of the Second Conference on\\nEmpirical Methods in Natural Language Processing, 81- 89. ACL Press.\\n\\n\\x0c'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
