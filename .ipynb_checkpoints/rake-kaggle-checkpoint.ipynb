{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year                                              title event_type  \\\n",
       "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
       "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
       "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
       "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
       "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
       "\n",
       "                                            pdf_name          abstract  \\\n",
       "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
       "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
       "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
       "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
       "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
       "\n",
       "                                          paper_text  \n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
       "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
       "4  Neural Network Ensembles, Cross\\nValidation, a...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# load the dataset\n",
    "df = pd.read_csv('papers.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Non-negative matrix factorization (NMF) has previously been shown to \" + \\\n",
    "\"be a useful decomposition for multivariate data. Two different multiplicative \" + \\\n",
    "\"algorithms for NMF are analyzed. They differ only slightly in the \" + \\\n",
    "\"multiplicative factor used in the update rules. One algorithm can be shown to \" + \\\n",
    "\"minimize the conventional least squares error while the other minimizes the  \" + \\\n",
    "\"generalized Kullback-Leibler divergence. The monotonic convergence of both  \" + \\\n",
    "\"algorithms can be proven using an auxiliary function analogous to that used \" + \\\n",
    "\"for proving convergence of the Expectation-Maximization algorithm. The algorithms  \" + \\\n",
    "\"can also be interpreted as diagonally rescaled gradient descent, where the  \" + \\\n",
    "\"rescaling factor is optimally chosen to ensure convergence.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16.0, 'diagonally rescaled gradient descent'),\n",
       " (16.0, 'conventional least squares error'),\n",
       " (14.0, 'two different multiplicative algorithms'),\n",
       " (9.0, 'negative matrix factorization'),\n",
       " (9.0, 'auxiliary function analogous'),\n",
       " (8.0, 'multiplicative factor used'),\n",
       " (4.5, 'rescaling factor'),\n",
       " (4.0, 'useful decomposition'),\n",
       " (4.0, 'update rules'),\n",
       " (4.0, 'proving convergence')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "r = Rake()\n",
    "r.extract_keywords_from_text(text)\n",
    "r.get_ranked_phrases_with_scores()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('PAPER_TEXT:Algorithms for Non-negative Matrix\\n'\n",
      " 'Factorization\\n'\n",
      " '\\n'\n",
      " 'Daniel D. Lee*\\n'\n",
      " '*BelJ Laboratories\\n'\n",
      " 'Lucent Technologies\\n'\n",
      " 'Murray Hill, NJ 07974\\n'\n",
      " '\\n'\n",
      " 'H. Sebastian Seung*t\\n'\n",
      " 'tDept. of Brain and Cog. Sci.\\n'\n",
      " 'Massachusetts Institute of Technology\\n'\n",
      " 'Cambridge, MA 02138\\n'\n",
      " '\\n'\n",
      " 'Abstract\\n'\n",
      " 'Non-negative matrix factorization (NMF) has previously been shown to\\n'\n",
      " 'be a useful decomposition for multivariate data. Two different '\n",
      " 'multiplicative algorithms for NMF are analyzed. They differ only slightly '\n",
      " 'in\\n'\n",
      " 'the multiplicative factor used in the update rules. One algorithm can be\\n'\n",
      " 'shown to minimize the conventional least squares error while the other\\n'\n",
      " 'minimizes the generalized Kullback-Leibler divergence. The monotonic\\n'\n",
      " 'convergence of both algorithms can be proven using an auxiliary function '\n",
      " 'analogous to that used for proving convergence of the '\n",
      " 'ExpectationMaximization algorithm. The algorithms can also be interpreted as '\n",
      " 'diagonally rescaled gradient descent, where the rescaling factor is '\n",
      " 'optimally\\n'\n",
      " 'chosen to ensure convergence.\\n'\n",
      " '\\n'\n",
      " '1 Introduction\\n'\n",
      " 'Unsupervised learning algorithms such as principal components analysis and '\n",
      " 'vector quantization can be understood as factorizing a data matrix subject '\n",
      " 'to different constraints. Depending upon the constraints utilized, the '\n",
      " 'resulting factors can be shown to have very different representational '\n",
      " 'properties. Principal components analysis enforces only a weak orthogonality '\n",
      " 'constraint, resulting in a very distributed representation that uses '\n",
      " 'cancellations\\n'\n",
      " 'to generate variability [1, 2]. On the other hand, vector quantization uses '\n",
      " 'a hard winnertake-all constraint that results in clustering the data into '\n",
      " 'mutually exclusive prototypes [3].\\n'\n",
      " 'We have previously shown that nonnegativity is a useful constraint for '\n",
      " 'matrix factorization\\n'\n",
      " 'that can learn a parts representation of the data [4, 5]. The nonnegative '\n",
      " 'basis vectors that are\\n'\n",
      " 'learned are used in distributed, yet still sparse combinations to generate '\n",
      " 'expressiveness in\\n'\n",
      " 'the reconstructions [6, 7]. In this submission, we analyze in detail two '\n",
      " 'numerical algorithms\\n'\n",
      " 'for learning the optimal nonnegative factors from data.\\n'\n",
      " '\\n'\n",
      " '2 Non-negative matrix factorization\\n'\n",
      " 'We formally consider algorithms for solving the following problem:\\n'\n",
      " 'Non-negative matrix factorization (NMF) Given a non-negative matrix\\n'\n",
      " 'V, find non-negative matrix factors Wand H such that:\\n'\n",
      " 'V~WH\\n'\n",
      " '\\n'\n",
      " '(1)\\n'\n",
      " '\\n'\n",
      " '\\x0c'\n",
      " 'NMF can be applied to the statistical analysis of multivariate data in the '\n",
      " 'following manner.\\n'\n",
      " 'Given a set of of multivariate n-dimensional data vectors, the vectors are '\n",
      " 'placed in the\\n'\n",
      " 'columns of an n x m matrix V where m is the number of examples in the data '\n",
      " 'set. This\\n'\n",
      " 'matrix is then approximately factorized into an n x r matrix Wand an r x m '\n",
      " 'matrix H.\\n'\n",
      " 'Usually r is chosen to be smaller than nor m , so that Wand H are smaller '\n",
      " 'than the original\\n'\n",
      " 'matrix V. This results in a compressed version of the original data matrix.\\n'\n",
      " 'What is the significance of the approximation in Eq. (1)? It can be '\n",
      " 'rewritten column by\\n'\n",
      " 'column as v ~ Wh, where v and h are the corresponding columns of V and H. In '\n",
      " 'other\\n'\n",
      " 'words, each data vector v is approximated by a linear combination of the '\n",
      " 'columns of W,\\n'\n",
      " 'weighted by the components of h. Therefore W can be regarded as containing a '\n",
      " 'basis\\n'\n",
      " 'that is optimized for the linear approximation of the data in V. Since '\n",
      " 'relatively few basis\\n'\n",
      " 'vectors are used to represent many data vectors, good approximation can only '\n",
      " 'be achieved\\n'\n",
      " 'if the basis vectors discover structure that is latent in the data.\\n'\n",
      " 'The present submission is not about applications of NMF, but focuses instead '\n",
      " 'on the technical aspects of finding non-negative matrix factorizations. Of '\n",
      " 'course, other types of matrix factorizations have been extensively studied '\n",
      " 'in numerical linear algebra, but the nonnegativity constraint makes much of '\n",
      " 'this previous work inapplicable to the present case\\n'\n",
      " '[8].\\n'\n",
      " '\\n'\n",
      " 'Here we discuss two algorithms for NMF based on iterative updates of Wand H. '\n",
      " 'Because\\n'\n",
      " 'these algorithms are easy to implement and their convergence properties are '\n",
      " 'guaranteed,\\n'\n",
      " 'we have found them very useful in practical applications. Other algorithms '\n",
      " 'may possibly\\n'\n",
      " 'be more efficient in overall computation time, but are more difficult to '\n",
      " 'implement and may\\n'\n",
      " 'not generalize to different cost functions. Algorithms similar to ours where '\n",
      " 'only one of the\\n'\n",
      " 'factors is adapted have previously been used for the deconvolution of '\n",
      " 'emission tomography\\n'\n",
      " 'and astronomical images [9, 10, 11, 12].\\n'\n",
      " 'At each iteration of our algorithms, the new value of W or H is found by '\n",
      " 'multiplying the\\n'\n",
      " 'current value by some factor that depends on the quality ofthe approximation '\n",
      " 'in Eq. (1). We\\n'\n",
      " 'prove that the quality of the approximation improves monotonically with the '\n",
      " 'application\\n'\n",
      " 'of these multiplicative update rules. In practice, this means that repeated '\n",
      " 'iteration of the\\n'\n",
      " 'update rules is guaranteed to converge to a locally optimal matrix '\n",
      " 'factorization.\\n'\n",
      " '\\n'\n",
      " '3 Cost functions\\n'\n",
      " 'To find an approximate factorization V ~ W H, we first need to define cost '\n",
      " 'functions\\n'\n",
      " 'that quantify the quality of the approximation. Such a cost function can be '\n",
      " 'constructed\\n'\n",
      " 'using some measure of distance between two non-negative matrices A and B . '\n",
      " 'One useful\\n'\n",
      " 'measure is simply the square of the Euclidean distance between A and B '\n",
      " '[13],\\n'\n",
      " '\\n'\n",
      " 'IIA - BI12 = L(Aij -\\n'\n",
      " '\\n'\n",
      " 'Bij)2\\n'\n",
      " '\\n'\n",
      " '(2)\\n'\n",
      " '\\n'\n",
      " 'ij\\n'\n",
      " '\\n'\n",
      " 'This is lower bounded by zero, and clearly vanishes if and only if A = B .\\n'\n",
      " 'Another useful measure is\\n'\n",
      " 'D(AIIB)\\n'\n",
      " '\\n'\n",
      " '=\\n'\n",
      " '\\n'\n",
      " '2:\\n'\n",
      " '\\n'\n",
      " 'k?\\n'\n",
      " '( Aij log B:~\\n'\n",
      " '- Aij\\n'\n",
      " '\\n'\n",
      " '+ Bij )\\n'\n",
      " '\\n'\n",
      " '(3)\\n'\n",
      " '\\n'\n",
      " '\"J\\n'\n",
      " '\\n'\n",
      " 'Like the Euclidean distance this is also lower bounded by zero, and vanishes '\n",
      " 'if and only\\n'\n",
      " 'if A = B . But it cannot be called a \"distance\", because it is not symmetric '\n",
      " 'in A and B,\\n'\n",
      " 'so we will refer to it as the \"divergence\" of A from B. It reduces to the '\n",
      " 'Kullback-Leibler\\n'\n",
      " 'divergence, or relative entropy, when 2:ij Aij = 2:ij Bij = 1, so that A and '\n",
      " 'B can be\\n'\n",
      " 'regarded as normalized probability distributions.\\n'\n",
      " '\\n'\n",
      " '\\x0c'\n",
      " 'We now consider two alternative formulations of NMF as optimization '\n",
      " 'problems:\\n'\n",
      " 'Problem 1 Minimize\\n'\n",
      " '\\n'\n",
      " 'IIV -\\n'\n",
      " '\\n'\n",
      " 'W\\n'\n",
      " '\\n'\n",
      " 'HI12 with\\n'\n",
      " '\\n'\n",
      " 'respect to Wand H, subject to the constraints\\n'\n",
      " '\\n'\n",
      " 'W,H~O.\\n'\n",
      " '\\n'\n",
      " 'Problem 2 Minimize D(VIIW H) with re.lpect to Wand H, subject to the '\n",
      " 'constraints\\n'\n",
      " 'W,H~O.\\n'\n",
      " '\\n'\n",
      " 'Although the functions IIV - W HI12 and D(VIIW H) are convex in W only or H '\n",
      " 'only, they\\n'\n",
      " 'are not convex in both variables together. Therefore it is unrealistic to '\n",
      " 'expect an algorithm\\n'\n",
      " 'to solve Problems 1 and 2 in the sense of finding global minima. However, '\n",
      " 'there are many\\n'\n",
      " 'techniques from numerical optimization that can be applied to find local '\n",
      " 'minima.\\n'\n",
      " 'Gradient descent is perhaps the simplest technique to implement, but '\n",
      " 'convergence can be\\n'\n",
      " 'slow. Other methods such as conjugate gradient have faster convergence, at '\n",
      " 'least in the\\n'\n",
      " 'vicinity of local minima, but are more complicated to implement than '\n",
      " 'gradient descent\\n'\n",
      " '[8] . The convergence of gradient based methods also have the disadvantage '\n",
      " 'of being very\\n'\n",
      " 'sensitive to the choice of step size, which can be very inconvenient for '\n",
      " 'large applications.\\n'\n",
      " '\\n'\n",
      " '4 Multiplicative update rules\\n'\n",
      " 'We have found that the following \"multiplicative update rules\" are a good '\n",
      " 'compromise\\n'\n",
      " 'between speed and ease of implementation for solving Problems 1 and 2.\\n'\n",
      " 'Theorem 1 The Euclidean distance II V - W H II is non increasing under the '\n",
      " 'update rules\\n'\n",
      " '(WTV)att\\n'\n",
      " \"Hal' +- Hal' (WTWH)att\\n\"\n",
      " '\\n'\n",
      " '(V HT)ia\\n'\n",
      " 'Wia +- Wia(WHHT)ia\\n'\n",
      " '\\n'\n",
      " '(4)\\n'\n",
      " '\\n'\n",
      " 'The Euclidean distance is invariant under these updates if and only if Wand '\n",
      " 'H are at a\\n'\n",
      " 'stationary point of the distance.\\n'\n",
      " '\\n'\n",
      " 'Theorem 2 The divergence D(VIIW H) is nonincreasing under the update rules\\n'\n",
      " 'H\\n'\n",
      " '\\n'\n",
      " 'att +-\\n'\n",
      " '\\n'\n",
      " 'H\\n'\n",
      " 'att\\n'\n",
      " '\\n'\n",
      " '2:i WiaVitt/(WH)itt\\n'\n",
      " '\" W\\n'\n",
      " 'L..Jk\\n'\n",
      " '\\n'\n",
      " 'ka\\n'\n",
      " '\\n'\n",
      " 'Wia +- Wia\\n'\n",
      " '\\n'\n",
      " \"2:1' HattVitt/(WH)itt\\n\"\n",
      " '\" H\\n'\n",
      " 'L..Jv\\n'\n",
      " '\\n'\n",
      " 'av\\n'\n",
      " '\\n'\n",
      " '(5)\\n'\n",
      " '\\n'\n",
      " 'The divergence is invariant under these updates if and only ifW and H are at '\n",
      " 'a stationary\\n'\n",
      " 'point of the divergence.\\n'\n",
      " '\\n'\n",
      " 'Proofs of these theorems are given in a later section. For now, we note that '\n",
      " 'each update\\n'\n",
      " 'consists of multiplication by a factor. In particular, it is straightforward '\n",
      " 'to see that this\\n'\n",
      " 'multiplicative factor is unity when V = W H, so that perfect reconstruction '\n",
      " 'is necessarily\\n'\n",
      " 'a fixed point of the update rules.\\n'\n",
      " '\\n'\n",
      " '5 Multiplicative versus additive update rules\\n'\n",
      " 'It is useful to contrast these multiplicative updates with those arising '\n",
      " 'from gradient descent\\n'\n",
      " '[14]. In particular, a simple additive update for H that reduces the squared '\n",
      " 'distance can be\\n'\n",
      " 'written as\\n'\n",
      " '\\n'\n",
      " '(6)\\n'\n",
      " \"If 'flatt are all set equal to some small positive number, this is \"\n",
      " 'equivalent to conventional\\n'\n",
      " 'gradient descent. As long as this number is sufficiently small, the update '\n",
      " 'should reduce\\n'\n",
      " 'IIV - WHII?\\n'\n",
      " '\\n'\n",
      " '\\x0c'\n",
      " 'Now if we diagonally rescale the variables and set\\n'\n",
      " '\\n'\n",
      " 'Halt\\n'\n",
      " '\\n'\n",
      " '\"Ialt\\n'\n",
      " '\\n'\n",
      " '(7)\\n'\n",
      " '\\n'\n",
      " \"= (WTW H)alt '\\n\"\n",
      " '\\n'\n",
      " 'then we obtain the update rule for H that is given in Theorem 1. Note that '\n",
      " 'this rescaling\\n'\n",
      " 'results in a multiplicative factor with the positive component of the '\n",
      " 'gradient in the denominator and the absolute value of the negative component '\n",
      " 'in the numerator of the factor.\\n'\n",
      " 'For the divergence, diagonally rescaled gradient descent takes the form\\n'\n",
      " '\\n'\n",
      " 'Halt\\n'\n",
      " '\\n'\n",
      " 'f-\\n'\n",
      " '\\n'\n",
      " 'Halt\\n'\n",
      " '\\n'\n",
      " '+ \"Ialt\\n'\n",
      " '\\n'\n",
      " '[~Wia (:;;)ilt - ~ Wia].\\n'\n",
      " '\\n'\n",
      " '(8)\\n'\n",
      " '\\n'\n",
      " 'Again, if the \"Ialt are small and positive, this update should reduce D (V '\n",
      " 'II W H). If we now\\n'\n",
      " 'set\\n'\n",
      " '\\n'\n",
      " 'Halt\\n'\n",
      " '\"Ialt= ui\\n'\n",
      " \"~ W. '\\n\"\n",
      " 'za\\n'\n",
      " '\\n'\n",
      " '(9)\\n'\n",
      " '\\n'\n",
      " 'then we obtain the update rule for H that is given in Theorem 2. This '\n",
      " 'rescaling can also\\n'\n",
      " 'be interpretated as a multiplicative rule with the positive component of the '\n",
      " 'gradient in the\\n'\n",
      " 'denominator and negative component as the numerator of the multiplicative '\n",
      " 'factor.\\n'\n",
      " 'Since our choices for \"Ialt are not small, it may seem that there is no '\n",
      " 'guarantee that such a\\n'\n",
      " 'rescaled gradient descent should cause the cost function to decrease. '\n",
      " 'Surprisingly, this is\\n'\n",
      " 'indeed the case as shown in the next section.\\n'\n",
      " '\\n'\n",
      " '6 Proofs of convergence\\n'\n",
      " 'To prove Theorems 1 and 2, we will make use of an auxiliary function similar '\n",
      " 'to that used\\n'\n",
      " 'in the Expectation-Maximization algorithm [15, 16].\\n'\n",
      " \"Definition 1 G(h, h') is an auxiliary functionfor F(h)\\n\"\n",
      " \"G(h, h') ~ F(h),\\n\"\n",
      " '\\n'\n",
      " 'G(h, h)\\n'\n",
      " '\\n'\n",
      " 'if the conditions\\n'\n",
      " '\\n'\n",
      " '= F(h)\\n'\n",
      " '\\n'\n",
      " '(10)\\n'\n",
      " '\\n'\n",
      " 'are satisfied.\\n'\n",
      " '\\n'\n",
      " 'The auxiliary function is a useful concept because of the following lemma, '\n",
      " 'which is also\\n'\n",
      " 'graphically illustrated in Fig. 1.\\n'\n",
      " 'Lemma 1 IfG is an auxiliary junction, then F is nonincreasing under the '\n",
      " 'update\\n'\n",
      " 'ht+1 = argmlnG (h,ht )\\n'\n",
      " 'Proof: F(ht+1) ~ G(ht+1, ht) ~ G(ht, ht)\\n'\n",
      " '\\n'\n",
      " '(11)\\n'\n",
      " '\\n'\n",
      " '= F(ht) ?\\n'\n",
      " '\\n'\n",
      " 'Note that F(ht+1) = F(ht) only if ht is a local minimum of G(h, ht). If the '\n",
      " 'derivatives\\n'\n",
      " 'of F exist and are continuous in a small neighborhood of ht , this also '\n",
      " 'implies that the\\n'\n",
      " \"derivatives 'V F(ht) = O. Thus, by iterating the update in Eq. (11) we \"\n",
      " 'obtain a sequence\\n'\n",
      " 'of estimates that converge to a local minimum h min = argminh F(h) of the '\n",
      " 'objective\\n'\n",
      " 'function:\\n'\n",
      " '\\n'\n",
      " 'We will show that by defining the appropriate auxiliary functions G(h, ht) '\n",
      " 'for both IIV W HII and D(V, W H), the update rules in Theorems 1 and 2 '\n",
      " 'easily follow from Eq. (11).\\n'\n",
      " '\\n'\n",
      " '\\x0c'\n",
      " 'Figure 1: Minimizing the auxiliary function G(h, ht)\\n'\n",
      " 'F(ht) for h n+1 = argminh G(h, ht).\\n'\n",
      " '\\n'\n",
      " '2:: F(h) guarantees that F(ht+1) :::;\\n'\n",
      " '\\n'\n",
      " 'Lemma 2 If K(ht) is the diagonal matrix\\n'\n",
      " 'Kab(ht) = <5ab(WTwht)a/h~\\n'\n",
      " 'then\\n'\n",
      " 'G(h, ht)\\n'\n",
      " '\\n'\n",
      " '= F(ht) + (h -\\n'\n",
      " '\\n'\n",
      " '+ ~(h -\\n'\n",
      " '\\n'\n",
      " 'ht)T\\\\l F(ht)\\n'\n",
      " '\\n'\n",
      " '(13)\\n'\n",
      " '\\n'\n",
      " 'ht)T K(ht)(h - ht)\\n'\n",
      " '\\n'\n",
      " '(14)\\n'\n",
      " '\\n'\n",
      " 'is an auxiliary function for\\n'\n",
      " 'F(h) =\\n'\n",
      " '\\n'\n",
      " '~ ~)Vi -\\n'\n",
      " '\\n'\n",
      " 'a\\n'\n",
      " '\\n'\n",
      " 'Proof: Since G(h, h) = F(h) is obvious, we need only show that G(h, ht)\\n'\n",
      " 'do this, we compare\\n'\n",
      " 'F(h) = F(ht)\\n'\n",
      " '\\n'\n",
      " '+ (h -\\n'\n",
      " '\\n'\n",
      " 'htf\\\\l F(ht)\\n'\n",
      " '\\n'\n",
      " '+ ~(h -\\n'\n",
      " '\\n'\n",
      " '2:: F(h). To\\n'\n",
      " '\\n'\n",
      " 'ht)T(WTW)(h - ht)\\n'\n",
      " '\\n'\n",
      " '2\\n'\n",
      " 'with Eq. (14) to find that G(h, ht) 2:: F(h) is equivalent to\\n'\n",
      " '\\n'\n",
      " '0:::;\\n'\n",
      " '\\n'\n",
      " '(15)\\n'\n",
      " '\\n'\n",
      " 'W ia h a )2\\n'\n",
      " '\\n'\n",
      " 'L\\n'\n",
      " '\\n'\n",
      " 'i\\n'\n",
      " '\\n'\n",
      " '(h - htf[K(ht) - WTW](h - ht)\\n'\n",
      " '\\n'\n",
      " '(16)\\n'\n",
      " '\\n'\n",
      " '(17)\\n'\n",
      " '\\n'\n",
      " 'To prove positive semidefiniteness, consider the matrix 1:\\n'\n",
      " '(18)\\n'\n",
      " '\\n'\n",
      " 'which is just a rescaling of the components of K - WTW. Then K - WTW is '\n",
      " 'positive\\n'\n",
      " 'semidefinite if and only if M is, and\\n'\n",
      " 'VT M v\\n'\n",
      " '\\n'\n",
      " '=\\n'\n",
      " '\\n'\n",
      " 'L VaMabVb\\n'\n",
      " 'ab\\n'\n",
      " '\\n'\n",
      " '(19)\\n'\n",
      " '\\n'\n",
      " 'L h~(WTW)abh~v~ - vah~(WTW)abh~Vb\\n'\n",
      " 'ab\\n'\n",
      " '\\n'\n",
      " '(20)\\n'\n",
      " '\\n'\n",
      " '\"\\n'\n",
      " 't t\\n'\n",
      " 'L...J(W T W ) abhahb\\n'\n",
      " '\\n'\n",
      " '[1 + 1\\n'\n",
      " '2\" v a2\\n'\n",
      " '\\n'\n",
      " '2\" Vb2 - VaVb ]\\n'\n",
      " '\\n'\n",
      " '(21)\\n'\n",
      " '\\n'\n",
      " 'ab\\n'\n",
      " '\\n'\n",
      " '= ~ L(WTW)abh~h~(va -\\n'\n",
      " '\\n'\n",
      " 'Vb)2\\n'\n",
      " '\\n'\n",
      " '(22)\\n'\n",
      " '\\n'\n",
      " 'ab\\n'\n",
      " '\\n'\n",
      " '> 0\\n'\n",
      " '\\n'\n",
      " '(23)\\n'\n",
      " '\\n'\n",
      " \"'One can also show that K - WTW is positive semidefinite by considering the \"\n",
      " 'matrix K (I1\\n'\n",
      " '2.\\n'\n",
      " 'Then v. /M(WT W ht ) a is a positive eigenvector of K- 21 W T W K- with\\n'\n",
      " 'unity eigenvalue, and application of the Frobenius-Perron theorem shows that '\n",
      " 'Eq. 17 holds.\\n'\n",
      " '\\n'\n",
      " 'K- 21 W TW K- 21) K\\n'\n",
      " '\\n'\n",
      " '\\x0c'\n",
      " '?\\n'\n",
      " '\\n'\n",
      " 'We can now demonstrate the convergence of Theorem 1:\\n'\n",
      " 'Proof of Theorem 1 Replacing G(h, ht) in Eq. (11) by Eq. (14) results in the '\n",
      " 'update rule:\\n'\n",
      " 'ht+1 = ht - K(ht)-l\\\\1F(ht)\\n'\n",
      " '(24)\\n'\n",
      " 'Since Eq. (14) is an auxiliary function, F is nonincreasing under this '\n",
      " 'update rule, according\\n'\n",
      " 'to Lemma 1. Writing the components of this equation explicitly, we obtain\\n'\n",
      " 'ht +1 = ht (WT V )a\\n'\n",
      " 'a\\n'\n",
      " 'a (WTWht)a .\\n'\n",
      " '\\n'\n",
      " '(25)\\n'\n",
      " '\\n'\n",
      " 'By reversing the roles of Wand H in Lemma 1 and 2, F can similarly be shown '\n",
      " 'to be\\n'\n",
      " 'nonincreasing under the update rules for W .?\\n'\n",
      " 'We now consider the following auxiliary function for the divergence cost '\n",
      " 'function:\\n'\n",
      " 'Lemma 3 Define\\n'\n",
      " 'G(h,ht)\\n'\n",
      " '\\n'\n",
      " '(26)\\n'\n",
      " '\\n'\n",
      " 'ia\\n'\n",
      " '\"\\n'\n",
      " 'Wiah~ (\\n'\n",
      " 'Wiah~ )\\n'\n",
      " '- ~ Vi,\"\", W - ht logWiaha -log,\"\", W - ht\\n'\n",
      " 'ia\\n'\n",
      " 'ub ,b b\\n'\n",
      " 'ub ,b b\\n'\n",
      " '\\n'\n",
      " '(27)\\n'\n",
      " '\\n'\n",
      " 'This is an auxiliary function for\\n'\n",
      " '\\n'\n",
      " 'F(h)\\n'\n",
      " '\\n'\n",
      " '=L\\n'\n",
      " '\\n'\n",
      " 'Vi log\\n'\n",
      " 'i\\n'\n",
      " '\\n'\n",
      " '(~ ~_\\n'\n",
      " 'a\\n'\n",
      " '\\n'\n",
      " \"'l,a\\n\"\n",
      " '\\n'\n",
      " 'h ) - Vi\\n'\n",
      " '\\n'\n",
      " '+ LWiaha\\n'\n",
      " '\\n'\n",
      " 'a\\n'\n",
      " '\\n'\n",
      " 'Proof: It is straightforward to verify that G(h, h) = F(h) . To show that '\n",
      " 'G(h, ht)\\n'\n",
      " 'we use convexity of the log function to derive the inequality\\n'\n",
      " 'W\\n'\n",
      " 'iaha\\n'\n",
      " '-log \"~ Wiaha ::; - \"\\n'\n",
      " '~\\n'\n",
      " 'Q a log - a\\n'\n",
      " '\\n'\n",
      " '(28)\\n'\n",
      " '\\n'\n",
      " 'a\\n'\n",
      " '\\n'\n",
      " '2: F(h),\\n'\n",
      " '(29)\\n'\n",
      " '\\n'\n",
      " 'Qa\\n'\n",
      " '\\n'\n",
      " 'a\\n'\n",
      " '\\n'\n",
      " 'which holds for all nonnegative Q a that sum to unity. Setting\\n'\n",
      " 'Wiah~\\n'\n",
      " '\\n'\n",
      " 'Q\\n'\n",
      " '\\n'\n",
      " 'a\\n'\n",
      " '\\n'\n",
      " '(30)\\n'\n",
      " '\\n'\n",
      " \"= 'ub\\n\"\n",
      " '\"\\'\" Wibhbt\\n'\n",
      " '\\n'\n",
      " 'we obtain\\n'\n",
      " '-log \"~ Wiaha ::; - \"~ \\'\"\\'\"Wiah~\\n'\n",
      " 'W- ht ( log Wiaha - log,\"\",Wiah~\\n'\n",
      " 'W- ht )\\n'\n",
      " 'a\\n'\n",
      " 'a ub ,b b\\n'\n",
      " 'ub ,b b\\n'\n",
      " '\\n'\n",
      " '(31)\\n'\n",
      " '\\n'\n",
      " 'From this inequality it follows that F(h) ::; G(h, ht) . ?\\n'\n",
      " 'Theorem 2 then follows from the application of Lemma 1:\\n'\n",
      " 'Proof of Theorem 2: The minimum of G(h, ht) with respect to h is determined '\n",
      " 'by setting\\n'\n",
      " 'the gradient to zero:\\n'\n",
      " '_dG---,(,---,h,_h--,-t) __ \"\\n'\n",
      " '_ Wiah~ 1\\n'\n",
      " '~v,\\n'\n",
      " 't\\n'\n",
      " 'dha\\n'\n",
      " '_\\n'\n",
      " ', ~b Wibhb ha\\n'\n",
      " '\\n'\n",
      " '\"W- - 0\\n'\n",
      " '\\n'\n",
      " '+~\\n'\n",
      " '\\n'\n",
      " ',-\\n'\n",
      " '\\n'\n",
      " 'za-\\n'\n",
      " '\\n'\n",
      " '(32)\\n'\n",
      " '\\n'\n",
      " 'Thus, the update rule of Eq. (11) takes the form\\n'\n",
      " 't+1\\n'\n",
      " 'ha\\n'\n",
      " '\\n'\n",
      " 'h~\"\\n'\n",
      " '\\n'\n",
      " 'Vi\\n'\n",
      " '\\n'\n",
      " '= ub\\n'\n",
      " '\\'\"\\'\" wkb ~\\n'\n",
      " '\\'\"\\'\" W-,b htbW ia ?\\n'\n",
      " 'i\\n'\n",
      " 'ub\\n'\n",
      " '\\n'\n",
      " '(33)\\n'\n",
      " '\\n'\n",
      " 'Since G is an auxiliary function, F in Eq. (28) is nonincreasing under this '\n",
      " 'update. Rewritten in matrix form, this is equivalent to the update rule in '\n",
      " 'Eq. (5). By reversing the roles of\\n'\n",
      " 'Hand W, the update rule for W can similarly be shown to be nonincreasing .?\\n'\n",
      " '\\n'\n",
      " '\\x0c'\n",
      " '7 Discussion\\n'\n",
      " 'We have shown that application of the update rules in Eqs. (4) and (5) are '\n",
      " 'guaranteed to\\n'\n",
      " 'find at least locally optimal solutions of Problems 1 and 2, respectively. '\n",
      " 'The convergence\\n'\n",
      " 'proofs rely upon defining an appropriate auxiliary function . We are '\n",
      " 'currently working to\\n'\n",
      " 'generalize these theorems to more complex constraints. The update rules '\n",
      " 'themselves are\\n'\n",
      " 'extremely easy to implement computationally, and will hopefully be utilized '\n",
      " 'by others for\\n'\n",
      " 'a wide variety of applications.\\n'\n",
      " 'We acknowledge the support of Bell Laboratories. We would also like to thank '\n",
      " 'Carlos\\n'\n",
      " 'Brody, Ken Clarkson, Corinna Cortes, Roland Freund, Linda Kaufman, Yann Le '\n",
      " 'Cun, Sam\\n'\n",
      " 'Rowei s, Larry Saul, and Margaret Wright for helpful discussions.\\n'\n",
      " '\\n'\n",
      " 'References\\n'\n",
      " '[1] Jolliffe, IT (1986). Principal Component Analysis. New York: '\n",
      " 'Springer-Verlag.\\n'\n",
      " '[2] Turk, M & Pentland, A (1991). Eigenfaces for recognition. J. Cogn. '\n",
      " 'Neurosci. 3, 71- 86.\\n'\n",
      " '[3] Gersho, A & Gray, RM (1992). Vector Quantization and Signal Compression. '\n",
      " 'Kluwer Acad.\\n'\n",
      " 'Press.\\n'\n",
      " '[4] Lee, DD & Seung, HS . Unsupervised learning by convex and conic coding '\n",
      " '(1997). Proceedings\\n'\n",
      " 'of the Conference on Neural Information Processing Systems 9, 515- 521.\\n'\n",
      " '[5] Lee, DD & Seung, HS (1999). Learning the parts of objects by '\n",
      " 'non-negative matrix factorization. Nature 401, 788- 791.\\n'\n",
      " '[6] Field, DJ (1994). What is the goal of sensory coding? Neural Comput. 6, '\n",
      " '559-601.\\n'\n",
      " '[7] Foldiak, P & Young, M (1995). Sparse coding in the primate cortex. The '\n",
      " 'Handbook of Brain\\n'\n",
      " 'Theory and Neural Networks, 895- 898. (MIT Press, Cambridge, MA).\\n'\n",
      " '[8] Press, WH, Teukolsky, SA, Vetterling, WT & Flannery, BP (1993). '\n",
      " 'Numerical recipes: the art\\n'\n",
      " 'of scientific computing. (Cambridge University Press, Cambridge, England).\\n'\n",
      " '[9] Shepp, LA & Vardi, Y (1982) . Maximum likelihood reconstruction for '\n",
      " 'emission tomography.\\n'\n",
      " 'IEEE Trans . MI-2, 113- 122.\\n'\n",
      " '[10] Richardson, WH (1972) . Bayesian-based iterative method of image '\n",
      " 'restoration. 1. Opt. Soc.\\n'\n",
      " 'Am. 62, 55- 59.\\n'\n",
      " '\\n'\n",
      " '[11] Lucy, LB (1974). An iterative technique for the rectification of '\n",
      " 'observed distributions. Astron.\\n'\n",
      " 'J. 74, 745- 754.\\n'\n",
      " '[12] Bouman, CA & Sauer, K (1996). A unified approach to statistical '\n",
      " 'tomography using coordinate\\n'\n",
      " 'descent optimization. IEEE Trans. Image Proc. 5, 480--492.\\n'\n",
      " '[13] Paatero, P & Tapper, U (1997). Least squares formulation of robust '\n",
      " 'non-negative factor analysis. Chemometr. Intell. Lab. 37, 23- 35.\\n'\n",
      " '[14] Kivinen, J & Warmuth, M (1997). Additive versus exponentiated gradient '\n",
      " 'updates for linear\\n'\n",
      " 'prediction. Journal of Tnformation and Computation 132, 1-64.\\n'\n",
      " '[15] Dempster, AP, Laird, NM & Rubin, DB (1977). Maximum likelihood from '\n",
      " 'incomplete data via\\n'\n",
      " 'the EM algorithm. J. Royal Stat. Soc. 39, 1-38.\\n'\n",
      " '[16] Saul, L & Pereira, F (1997). Aggregate and mixed-order Markov models '\n",
      " 'for statistical language\\n'\n",
      " 'processing. In C. Cardie and R. Weischedel (eds). Proceedings of the Second '\n",
      " 'Conference on\\n'\n",
      " 'Empirical Methods in Natural Language Processing, 81- 89. ACL Press.\\n'\n",
      " '\\n'\n",
      " '\\x0c')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "sample = 941\n",
    "pprint.pprint(\"PAPER_TEXT:{}\".format(df['paper_text'][sample]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_rake(docs, n=10):\n",
    "    # Uses stopwords for english from NLTK, and all puntuation characters by default\n",
    "    r = Rake()\n",
    "    \n",
    "    # Extraction given the text.\n",
    "    r.extract_keywords_from_text(docs[1000:2000])\n",
    "    \n",
    "    # To get keyword phrases ranked highest to lowest.\n",
    "    keywords = r.get_ranked_phrases()[0:n]\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "def print_results(idx,keywords, df):\n",
    "    # now print the results\n",
    "    print(\"\\n=====Title=====\")\n",
    "    print(df['title'][idx])\n",
    "    print(\"\\n=====Abstract=====\")\n",
    "    print(df['abstract'][idx])\n",
    "    print(\"\\n===Keywords===\")\n",
    "    for k in keywords:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Title=====\n",
      "Algorithms for Non-negative Matrix Factorization\n",
      "\n",
      "=====Abstract=====\n",
      "Non-negative matrix factorization (NMF) has previously been shown to \r\n",
      "be a useful decomposition for multivariate data. Two different multi- \r\n",
      "plicative algorithms for NMF are analyzed. They differ only slightly in \r\n",
      "the multiplicative factor used in the update rules. One algorithm can be \r\n",
      "shown to minimize the conventional least squares error while the other \r\n",
      "minimizes the generalized Kullback-Leibler divergence. The monotonic \r\n",
      "convergence of both algorithms can be proven using an auxiliary func- \r\n",
      "tion analogous to that used for proving convergence of the Expectation- \r\n",
      "Maximization algorithm. The algorithms can also be interpreted as diag- \r\n",
      "onally rescaled gradient descent, where the rescaling factor is optimally \r\n",
      "chosen to ensure convergence. \n",
      "\n",
      "===Keywords===\n",
      "yet still sparse combinations\n",
      "detail two numerical al\n",
      "principal components analysis enforces\n",
      "principal components analysis\n",
      "pervised learning algorithms\n",
      "nonnegative basis vectors\n",
      "mutually exclusive prototypes\n",
      "different representational properties\n",
      "weak orthogonality constraint\n",
      "vector quantization uses\n"
     ]
    }
   ],
   "source": [
    "idx=941\n",
    "keywords = get_keywords_rake(df['paper_text'][941], n=10)\n",
    "print_results(idx, keywords, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
